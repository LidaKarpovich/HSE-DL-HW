{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KqgfdcFyqu1"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "\n",
        "Имя, Фамилия:\n",
        "Лидия Карпович\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8whqGqYigwJ",
        "outputId": "84930575-f20c-4c56-d982-9de8ad988f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[all] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (0.11.0)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[all])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[all])\n",
            "  Using cached swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\n",
            "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl.metadata (669 bytes)\n",
            "Collecting cython<3 (from gymnasium[all])\n",
            "  Using cached Cython-0.29.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[all])\n",
            "  Using cached mujoco-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (2.37.0)\n",
            "Requirement already satisfied: jax>=0.4.16 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (0.5.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.16 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (0.5.1)\n",
            "Requirement already satisfied: flax>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (0.10.6)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (2.6.0+cu124)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (3.10.0)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[all]) (1.0.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (0.11.13)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.5.0->gymnasium[all]) (0.1.9)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[all]) (11.2.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.16->gymnasium[all]) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.16->gymnasium[all]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.16->gymnasium[all]) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[all]) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.12)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[all]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[all]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[all])\n",
            "  Using cached glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[all]) (3.1.9)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.11/dist-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.17.1)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\n",
            "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->gymnasium[all])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->gymnasium[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->gymnasium[all]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.5.0->gymnasium[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.5.0->gymnasium[all]) (2.19.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[all]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[all]) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->gymnasium[all]) (3.0.2)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.5.0->gymnasium[all]) (0.1.89)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.5.0->gymnasium[all]) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.5.0->gymnasium[all]) (5.29.4)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.5.0->gymnasium[all]) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.5.0->gymnasium[all]) (3.20.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.5.0->gymnasium[all]) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.5.0->gymnasium[all]) (0.1.2)\n",
            "Using cached swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "Using cached Cython-0.29.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Using cached mujoco-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Using cached glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GBvz_TKCi-ZY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OiflUgtf5nJi"
      },
      "outputs": [],
      "source": [
        "# utils, нужно чтобы рендерить видео траекторий в colab\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# source: https://stackoverflow.com/a/69990457\n",
        "def show_video(video_path, video_width = 600):\n",
        "    video_file = open(video_path, \"r+b\").read()\n",
        "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyLk8-uYhzy1"
      },
      "source": [
        "## Введение в Gymnasium\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0GTR-Wyg0tM"
      },
      "source": [
        "Документация: https://gymnasium.farama.org\n",
        "\n",
        "Изначально появился под именем [Gym](https://github.com/openai/gym) (и так и остался), стал де-факто стандартом взаимодействия с различными средами и симуляторами для RL исследователей и инженеров. С тех пор все новые среды оформляют похожим образом. Удобно, т.к. уже написанные для Gym алгоритмы могут работать на любых новых средах, достаточно лишь, чтобы они соответствовали API.\n",
        "\n",
        "Тем не менее, спустя время OpenAI перестали его поддерживать и обновлять, накопилось багов, поэтому появился [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) - форк который поддерживается [комьюнити](https://farama.org) и постоянно обновляется. По сравнению с замороженным Gym было добавлено несколько больших изменений, ломающих обратную совместимость, поэтому сейчас Gym считается `deprecated` и весь новый код нужно писать на Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9I6XIB19h5px"
      },
      "outputs": [],
      "source": [
        "# по привычке импортирут под старым именем\n",
        "import gymnasium as gym\n",
        "\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5UdiWOlqk8"
      },
      "source": [
        "Gymnasium предоставляет некоторое количество стандартных сред (полный список можно посмотреть в документации). Сторонние библиотеки как правило регистрируют свои среды в Gym, поэтому в идеале для них ничего не менятся (если разрабы все сделали правильно).\n",
        "\n",
        "Рассмотрим какую-нибудь поближе, например классическую [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/):\n",
        "\n",
        "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\" width=\"300\">\n",
        "\n",
        "**NB**! Как правило одна и та же среда разных версий может различаться, поэтому на это стоит обращать внимание (особенно если воспроизводите чужой метод, результаты на разных версиях могут отличаться)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q_Y0FLoC7LFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671a9e14-c8fe-42f5-9252-b5a745b5f09e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZK1_Mg9B2uL"
      },
      "source": [
        "Глобально, среда определяется двумя вещами:\n",
        "\n",
        "1. **observation space** - пространство возможных состояний;\n",
        "2. **action space** - пространство возможных действий.\n",
        "\n",
        "В зависимости от пространства состояний может меняться архитектура агента и сложность задачи. С действиями еще сложнее, т.к. некоторые методы работают только с дискретными действиями (например, классический DQN), некоторые только с вещественными (например, DDPG, SAC). К счастью, бывают алгоритмы которые умеют и так и так (например, PPO).\n",
        "\n",
        "Задача же определяется с помощью награды (в целом очевидно)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I0B5LnAPA8TO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5a25d0-359a-4ccc-8d99-a5fa8814a863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Acton space: Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Acton space:\", env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AQsNqLQJDE-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec7d72a-4330-4122-9f64-25142da5d9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obs shape (4,)\n",
            "Obs min values [-4.8               -inf -0.41887903        -inf]\n",
            "Obs max values [4.8               inf 0.41887903        inf]\n"
          ]
        }
      ],
      "source": [
        "print(\"Obs shape\", env.observation_space.shape)\n",
        "print(\"Obs min values\", env.observation_space.low)\n",
        "print(\"Obs max values\", env.observation_space.high)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyqUv9AGFj4h"
      },
      "source": [
        "Взаимодействие с средой происходит через два основных метода:\n",
        "1. env**.reset()**\n",
        "2. env**.step()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b4iCmiQrFZPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a675ea-2205-4622-bf5c-90b93a1aa8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00114413 0.0005388  0.00693469 0.04288638]\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "# возвращает среду к начальному состоянию -> начинается новый эпизод\n",
        "reset_obs, reset_info = env.reset()\n",
        "\n",
        "print(reset_obs)\n",
        "print(reset_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J8EZXiG8F9Mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ba29a3-b391-457b-b9eb-498721c467d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.0011549  -0.1946819   0.00779242  0.33774918]\n",
            "1.0\n",
            "False\n",
            "False\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "next_obs, reward, terminated, truncated, info = env.step(action=0)\n",
        "\n",
        "print(next_obs)     # следующее состояние, в которое перешла среда после совершения действия\n",
        "print(reward)       # полученная награда\n",
        "print(terminated)   # считается ли задача выполненной\n",
        "print(truncated)    # нужно ли закончить эпизод по любой другой причине (например лимит кол-во шагов в одном эпизоде)\n",
        "print(info)         # дополнителные логи, если есть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SN4t0l6GrkZ"
      },
      "source": [
        "В целом, весь эпизод обычно выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dDRjvoJ9JMvK"
      },
      "outputs": [],
      "source": [
        "!rm -rf videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sA391LrXhIMF"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# это wrapper, с помощью них можно писать обертки, которые добавляют какие-то новые фичи поверх\n",
        "# в данном случае сохраняет видео эпизода, полезно для дебага и просто ради любопытства, посмотреть, что агент там делает\n",
        "env = RecordVideo(env, \"videos\", episode_trigger=lambda t: True, disable_logger=True)\n",
        "\n",
        "done = False # если True, то эпизод закончился\n",
        "obs, info = env.reset()\n",
        "\n",
        "while not done:\n",
        "    # пока что случайное действие\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "# не всегда нужно, но лучше так делать\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FxOGDewm6Svk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "85d3dbd3-e612-4b65-ed67-b23b039d55ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=600 controls><source src=\"data:video/mp4;base64,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\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "show_video(\"videos/rl-video-episode-0.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1XW8zF7RJI0"
      },
      "source": [
        "Краткий экскурс закончен, погнали уже что-то делать ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19jHfObgxW-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DklGRD2zJXEQ"
      },
      "source": [
        "## Задание 1: REINFORCE (3 балла)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pAyPqS9hDNM"
      },
      "source": [
        "Попробуем написать разобранный на лекции простой RL алгоритм - REINFORCE. Он умеет и в дискретные и вещественные действия (достаточно поменять Categorical распределение на Normal в акторе).\n",
        "\n",
        "Более современные on-policy методы, такие как A2C, PPO, PPG строятся по похожему принципу (меняются только лоссы, и то как собираются данные), поэтому важно понять общую идею."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6SlAo1qMty2",
        "outputId": "61e99f9a-e639-40a9-f9df-89deb508335c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlEDfFUuLQco"
      },
      "source": [
        "Начнем с актора $\\pi(a | s)$. В данном случае нам хватит маленькой трехслойной сети. На выходе мы хотим получать категориальное распределение - вероятности действий.\n",
        "\n",
        "Удобно также отдельно написать метод, который будет возвращать конкретное действие для стейта. Обычно такой метод не предполагает батчей, а работает сразу с np.ndarray, чтобы скрыть детали реализации (т.к. они бывают разные у разных методов)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mWf6Q3EfLQAz"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 32):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Политика для агента.\n",
        "        Args:\n",
        "            obs_dim (int): Размерность пространства наблюдений.\n",
        "            action_dim (int): Количество возможных дискретных действий.\n",
        "            hidden_dim (int): Количество нейронов в скрытом слое.\n",
        "        \"\"\"\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> Categorical:\n",
        "        \"\"\"\n",
        "        Прямой проход: возвращает категориальное распределение над действиями.\n",
        "\n",
        "        Args:\n",
        "            obs (torch.Tensor): Наблюдение (размер [batch_size, obs_dim]).\n",
        "\n",
        "        Returns:\n",
        "            Categorical: Распределение над действиями.\n",
        "        \"\"\"\n",
        "        logits = self.net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_action(self, obs: np.ndarray, greedy: bool = False) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает действие на основе текущей политики.\n",
        "\n",
        "        Args:\n",
        "            obs (np.ndarray): Наблюдение размером (obs_dim,)\n",
        "            greedy (bool): Если True — вернуть наиболее вероятное действие.\n",
        "\n",
        "        Returns:\n",
        "            int: Индекс выбранного действия.\n",
        "        \"\"\"\n",
        "        obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)  # добавляем batch размерность\n",
        "        dist = self.forward(obs_tensor)\n",
        "        if greedy:\n",
        "            action = torch.argmax(dist.probs, dim=-1)\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "        return action.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RA9rTm_hi0x"
      },
      "source": [
        "Дальше займемся сбором данных для обучения. Последовательно (в наивной имплементации, как делать лучше обсудим чуть позже) соберем $N$ эпизодов, сохраним все что нужно для обучения - состояния, действия, суммарные награды за эпизоды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g7gqB0bRhc5T"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict\n",
        "\n",
        "def collect_experience(env: gym.Env,\n",
        "                       actor: Actor,\n",
        "                       num_episodes: int) -> Tuple[torch.Tensor,\n",
        "                                                   torch.Tensor,\n",
        "                                                   torch.Tensor,\n",
        "                                                   Dict]:\n",
        "    \"\"\"\n",
        "    Собирает опыт (траектории) от взаимодействия актора с окружением.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Среда Gym, с которой взаимодействует агент.\n",
        "        actor (Actor): Политика (актор), которая выбирает действия на основе наблюдений.\n",
        "        num_episodes (int): Количество эпизодов, которые нужно собрать.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:\n",
        "            - batch_obs: Батч наблюдений.\n",
        "            - batch_actions: Батч действий.\n",
        "            - batch_scores: Веса для градиента политики — суммарная награда эпизода,\n",
        "                            повторенная для каждого шага эпизода.\n",
        "            - info (dict): Дополнительная информация, например:\n",
        "                - 'mean_return': средняя суммарная награда на эпизод,\n",
        "                - 'mean_len': средняя длина эпизода.\n",
        "\n",
        "    Notes:\n",
        "        Для каждого шага в эпизоде сохраняются наблюдение и действие.\n",
        "        В конце эпизода суммарная награда эпизода (episode_reward) повторяется episode_len раз\n",
        "        и добавляется как вес для policy gradient.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_obs = []          # for observations\n",
        "    batch_actions = []      # for actions\n",
        "    batch_scores = []       # for R(tau) weighting in policy gradient\n",
        "\n",
        "    returns, lengths = [], []\n",
        "    for _ in range(num_episodes):\n",
        "        obs, done = env.reset()[0], False\n",
        "\n",
        "        episode_reward, episode_len = 0, 0\n",
        "        while not done:\n",
        "            action = actor.get_action(obs)\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            batch_obs.append(obs)\n",
        "            batch_actions.append(action)\n",
        "            episode_reward += reward\n",
        "            episode_len += 1\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "        # the weight for each logprob(a|s) is R(tau)\n",
        "        batch_scores += [episode_reward] * episode_len\n",
        "        returns.append(episode_reward)\n",
        "        lengths.append(episode_len)\n",
        "\n",
        "    info = {\n",
        "        \"mean_return\": np.mean(returns),\n",
        "        \"mean_len\": np.mean(lengths)\n",
        "    }\n",
        "    return (\n",
        "        torch.tensor(np.array(batch_obs), dtype=torch.float, device=device),\n",
        "        torch.tensor(np.array(batch_actions), dtype=torch.float, device=device),\n",
        "        torch.tensor(np.array(batch_scores), dtype=torch.float, device=device),\n",
        "        info\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fn3ghZgiHfk"
      },
      "source": [
        "Определим наш лосс. В отличие от лоссов привычных в DL, данный **лосс сам по себе не имеет интерпретации** и не стоит пытаться делать каких-то выводов на основе его значений. Для нас важно то, что градиент от этого лосса совпадает с policy gradient - то что нам нужно для обновления весов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EfXU7r8XiFoh"
      },
      "outputs": [],
      "source": [
        "def policy_gradient_loss(log_prob: torch.Tensor,\n",
        "                         score: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисляет loss.\n",
        "\n",
        "    Args:\n",
        "        log_probs (torch.Tensor): Логарифмированные вероятности действий.\n",
        "                                  shape: [batch_size]\n",
        "        score (torch.Tensor): Оценки, на основе которых происходит взвешивание\n",
        "                              лог-вероятностей.\n",
        "                              shape: [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Значение loss.\n",
        "    \"\"\"\n",
        "    return -(log_prob * score).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvPF9wRirDL"
      },
      "source": [
        "Осталось собрать все вместе. На каждой эпохе будем собирать `batch_episodes` эпизодов, пересчитывать $\\log \\pi(a | s)$, обновлять веса обычным для PyTorch способом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5a4QQHaMRJX2"
      },
      "outputs": [],
      "source": [
        "def train(actor: Actor,\n",
        "          env_name: str,\n",
        "          lr: float = 1e-3,\n",
        "          epochs: int = 1000,\n",
        "          batch_episodes: int = 10):\n",
        "    \"\"\"\n",
        "    Обучение агента.\n",
        "\n",
        "    Args:\n",
        "        actor (Actor): Политика агента.\n",
        "        env_name (str): Название среды Gym для обучения.\n",
        "        lr (float, optional): Скорость обучения для оптимизатора.\n",
        "        epochs (int, optional): Количество эпох обучения.\n",
        "        batch_episodes (int, optional): Количество эпизодов на одну итерацию обучения.\n",
        "    \"\"\"\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"This example only works for discrete action spaces.\"\n",
        "    optim = torch.optim.Adam(actor.parameters(), lr=lr)\n",
        "\n",
        "    for i in trange(epochs, desc=\"Epochs\"):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        lengths = []\n",
        "\n",
        "        for _ in range(batch_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            ep_rewards = []\n",
        "            ep_log_probs = []\n",
        "\n",
        "            while not done:\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "                dist = actor(obs_tensor)\n",
        "                action = dist.sample()\n",
        "                log_prob = dist.log_prob(action)\n",
        "\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                done = terminated or truncated\n",
        "                obs = next_obs\n",
        "\n",
        "\n",
        "                ep_rewards.append(reward)\n",
        "                ep_log_probs.append(log_prob)\n",
        "\n",
        "            total_reward = sum(ep_rewards)\n",
        "            episode_len = len(ep_rewards)\n",
        "            rewards.extend([total_reward] * episode_len)\n",
        "            log_probs.extend(ep_log_probs)\n",
        "            lengths.append(episode_len)\n",
        "\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        loss = policy_gradient_loss(log_probs, rewards)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            mean_return = sum(rewards.tolist()) / batch_episodes\n",
        "            mean_len = sum(lengths) / batch_episodes\n",
        "            tqdm.write(f\"Epoch {i}. Mean return: {mean_return:.2f}, Mean length: {mean_len:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389,
          "referenced_widgets": [
            "d0d2028f83c04af5a2689a629bcf7327",
            "fa34922a482249298214b7c4d857d504",
            "fbb68225643e4e91ab199ab4d1fd2f2c",
            "9c87ace7df6d47488889865866031bff",
            "429624396b1f478bb12de2214744043f",
            "b457cc115720488f823d9629cb67c4a5",
            "282eea743ba2424aa824cda7e3225126",
            "7daeb413355246eaa0f6125182a9bc8e",
            "c8a9fb652fea4ff6b37f7f821bb45628",
            "00394cb67edd4c05b51665c5f23c7829",
            "bf75b5a1526948e6bdfa32fefbe968f1"
          ]
        },
        "id": "1sKYm6tFgNCX",
        "outputId": "7e2a4448-5205-433e-e6eb-6f6ac4483eda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0d2028f83c04af5a2689a629bcf7327"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Mean return: 413.80, Mean length: 18.8\n",
            "Epoch 10. Mean return: 554.80, Mean length: 21.8\n",
            "Epoch 20. Mean return: 259.60, Mean length: 15.4\n",
            "Epoch 30. Mean return: 233.20, Mean length: 14.7\n",
            "Epoch 40. Mean return: 161.04, Mean length: 12.5\n",
            "Epoch 50. Mean return: 140.36, Mean length: 11.7\n",
            "Epoch 60. Mean return: 144.12, Mean length: 11.9\n",
            "Epoch 70. Mean return: 101.04, Mean length: 10.0\n",
            "Epoch 80. Mean return: 88.32, Mean length: 9.4\n",
            "Epoch 90. Mean return: 87.24, Mean length: 9.3\n",
            "Epoch 100. Mean return: 90.36, Mean length: 9.5\n",
            "Epoch 110. Mean return: 88.32, Mean length: 9.4\n",
            "Epoch 120. Mean return: 84.28, Mean length: 9.2\n",
            "Epoch 130. Mean return: 88.92, Mean length: 9.4\n",
            "Epoch 140. Mean return: 91.04, Mean length: 9.5\n",
            "Epoch 150. Mean return: 91.36, Mean length: 9.5\n",
            "Epoch 160. Mean return: 86.96, Mean length: 9.3\n",
            "Epoch 170. Mean return: 83.08, Mean length: 9.1\n",
            "Epoch 180. Mean return: 90.36, Mean length: 9.5\n",
            "Epoch 190. Mean return: 93.56, Mean length: 9.6\n"
          ]
        }
      ],
      "source": [
        "actor = Actor(env.observation_space.shape[-1], env.action_space.n)\n",
        "\n",
        "train(actor, \"CartPole-v1\", epochs=200, batch_episodes=25, lr=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZTeZYra07Cc"
      },
      "source": [
        "Попробуем визуализировать обученного агента"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TErgk1_T8TPR"
      },
      "outputs": [],
      "source": [
        "!rm -rf videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OIQvO0e68VMM"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "def rollout(env_name: str, agent: Actor) -> float:\n",
        "    \"\"\"\n",
        "    Выполняет один эпизод взаимодействия агента со средой.\n",
        "\n",
        "    Args:\n",
        "        env_name (str): Название среды Gym, которую нужно использовать.\n",
        "        agent (Actor): Объект агента, реализующий метод `get_action`.\n",
        "\n",
        "    Returns:\n",
        "        float: Общая награда, полученная за эпизод.\n",
        "    \"\"\"\n",
        "\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    video_dir = f\"videos/{uuid.uuid4()}\"\n",
        "    env = RecordVideo(env, video_dir, episode_trigger=lambda t: True, disable_logger=True)\n",
        "\n",
        "    done = False\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    total_reward = 0.0\n",
        "    while not done:\n",
        "        action = agent.get_action(obs, greedy=True)\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "    env.close()\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_H4f-QD_RZW",
        "outputId": "a43cc3be-19eb-4dd4-eba1-a1f9caad7a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "rollout(\"CartPole-v1\", actor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "piaXppHt1EpM",
        "outputId": "d5f58e8f-a62f-4e90-b5a2-7c0bcd03413e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=600 controls><source src=\"data:video/mp4;base64,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\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "show_video(\"videos/rl-video-episode-0.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4B6KXa4nITD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uw3nEIaKpKt"
      },
      "source": [
        "### Бонусная часть: StableBaselines3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ue9ApBcduvm"
      },
      "source": [
        "[StableBaselines3](https://stable-baselines3.readthedocs.io/en/master/) - наверное, самая популярная библиотека для RL. Подходит тем, кто хочет применять популярные RL бейзлайны к своим задачам, но не хочет лезть внутрь. В целом кастомизация сильно ограничена, но поменять архитектуру агента в некоторых случаях возможно.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4dy7ELleMfn"
      },
      "source": [
        "Часто лучший способ проверить насколько сложная у вас задача - попробовать несколько разных методов от туда, потюнить параметры. Потом уже решать, стоит ли коммититься, на то, чтобы написать свою имплементацию более сложного (или кастомизируемого) метода."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNs4wIhlTRx9",
        "outputId": "26840fb1-1dd2-4e02-ad39-f82ce5d79022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting pyglet\n",
            "  Downloading pyglet-2.1.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Downloading pyglet-2.1.6-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m984.0/984.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-2.1.6\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyglet rich tqdm\n",
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DzRpLxoEKoLB"
      },
      "outputs": [],
      "source": [
        "import gym as oldgym\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtZD1ptXWQZT"
      },
      "source": [
        "В отличие от нашей наивной реализации REINFORCE, где данные собираются последовательно, никакой настоящей нужды в таком подходе нет. Мы можем собирать эпизоды параллельно сразу с нескольких сред (в реальных задачах может доходить до десяток тысяч и больше)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPEe66FYepg1"
      },
      "source": [
        "В stable-baselines для этого уже написаны готовые обертки (как и в обычном [gymnasium](https://gymnasium.farama.org/api/vector/)).В остальном примерение алгоритмов максимально простое и заключается в основном в чтении документации, чтобы понять какие параметры там есть, и какие нужны и полезны под конкретную задачу.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAtB9-Uve8gE"
      },
      "source": [
        "Попробуем решить ту же среду с помощью DQN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "f25c7945b82b4da4bddefe2d4bf6384f",
            "fec07d495b034e01af7dda7e98747be5"
          ]
        },
        "id": "arva2aUgQSNK",
        "outputId": "36fdb52f-fae7-407e-84f1-706a96816d34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f25c7945b82b4da4bddefe2d4bf6384f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.11/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: \n",
              "Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n",
              "  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.11/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: \n",
              "Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n",
              "  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x79472000bdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "env = make_vec_env(\"CartPole-v1\", n_envs=8, seed=0, vec_env_cls=DummyVecEnv)\n",
        "\n",
        "model = DQN(\"MlpPolicy\", env)\n",
        "model.learn(total_timesteps=100_000, progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDSclNajl8bX"
      },
      "source": [
        "Работает это все шустрее и лучше, чем базовый REINFORCE *(и надо сильно меньше кода)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mvcwozLvQS_B"
      },
      "outputs": [],
      "source": [
        "eval_env = oldgym.make(\"CartPole-v1\")\n",
        "\n",
        "obs, done = eval_env.reset(), False\n",
        "total_reward = 0.0\n",
        "while not done:\n",
        "    action = model.predict(obs)[0].item()\n",
        "    obs, reward, done, _ = eval_env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC1YhbZ1fH04"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go3Vex4vpA85"
      },
      "source": [
        "### Дополнительные задания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAxJmUjBoPjS"
      },
      "source": [
        "\n",
        "\n",
        "*Каждый пункт дает 1 дополнительный балл.*\n",
        "\n",
        "1. Попробовать discounted return вместо суммарной награды за эпизод. Для этого придется суммировать дисконтированную награду (домноженную на $\\gamma$ в нужной степени)\n",
        "\n",
        "    P.S. По честному, для дисконтированной награды policy gradient [чуть чуть меняется](https://stanford.edu/~ashlearn/RLForFinanceBook/PolicyGradient.pdf), но на практике на это обычно закрывают глаза и используют формулу, которую мы вывели без учета дисконтирования. Это в целом интересная история (См. [Why there is a problem with the Policy Gradient theorem in Deep Reinforcement Learning](https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1))\n",
        "\n",
        "2. Попробовать вместо награды за всю траекторию использовать discounted return-to-go, то есть награду начиная с этого шага и дальше (См. [тут](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) секцию **Don’t Let the Past Distract You**)\n",
        "\n",
        "    Понимаете ли вы почему так правильнее? Как это отразится на скорости сходимости? Для сравнения имеет смысл логгировать награды, чтобы можно было построить график обучения (кол-во эпизодов по оси х, награда по y).\n",
        "\n",
        "3. Попробовать учить value function $V(s)$ для baseline (См. [тут](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) секцию **Baselines in Policy Gradients**).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "env = make_vec_env(\"CartPole-v1\", n_envs=8, seed=0)\n",
        "model = DQN(\"MlpPolicy\", env)\n",
        "model.learn(total_timesteps=100_000, progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50,
          "referenced_widgets": [
            "a8791418dcca4af88dad9a49319a7413",
            "c4d6dfd41e094225b727b3decbf97b65"
          ]
        },
        "id": "h2l3hgi1_0Iv",
        "outputId": "3b09f232-59cb-4b8a-b226-2234eab1e622"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8791418dcca4af88dad9a49319a7413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x794729b36910>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def compute_returns(rewards: List[float], gamma: float = 0.99) -> List[float]:\n",
        "    \"\"\"\n",
        "    Вычисляет return-to-go для каждого шага в эпизоде.\n",
        "\n",
        "    Args:\n",
        "        rewards (List[float]): список наград за эпизод.\n",
        "        gamma (float): коэффициент дисконтирования.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: список return-to-go, по одному на каждый шаг.\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    G = 0.0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    return returns"
      ],
      "metadata": {
        "id": "RI-yAStH_3PL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(actor: Actor, env_name: str, lr=1e-2, epochs=200, batch_episodes=25, gamma=0.99):\n",
        "    optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
        "    all_mean_rewards = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_obs = []\n",
        "        batch_actions = []\n",
        "        batch_returns = []\n",
        "        total_rewards = []\n",
        "\n",
        "        for episode in range(batch_episodes):\n",
        "            env = gym.make(env_name)\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "            rewards = []\n",
        "            log_probs = []\n",
        "            while not done:\n",
        "                action, log_prob = actor.get_action(obs, return_log_prob=True)\n",
        "                new_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                batch_obs.append(obs)\n",
        "                batch_actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                log_probs.append(log_prob)\n",
        "\n",
        "                obs = new_obs\n",
        "                done = terminated or truncated\n",
        "\n",
        "            total_rewards.append(sum(rewards))\n",
        "            returns = compute_returns(rewards, gamma=gamma)\n",
        "            batch_returns.extend(returns)\n",
        "\n",
        "        returns_tensor = torch.tensor(batch_returns, dtype=torch.float32)\n",
        "        returns_tensor = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 1e-8)\n",
        "\n",
        "        log_probs_tensor = torch.stack(log_probs)\n",
        "        loss = -torch.sum(log_probs_tensor * returns_tensor) / len(batch_returns)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mean_reward = np.mean(total_rewards)\n",
        "        all_mean_rewards.append(mean_reward)\n",
        "        print(f\"Epoch {epoch+1}: mean reward = {mean_reward:.2f}\")\n",
        "\n",
        "    return all_mean_rewards\n"
      ],
      "metadata": {
        "id": "NVo0xIWABKyP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ipLK8CnF9E"
      },
      "source": [
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UakXicKXqdUc"
      },
      "source": [
        "## Задание 2: Q-learning (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdPlm4-BqzjN"
      },
      "source": [
        "Обучите алгоритм Q-learning для [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) (1 балл) и [Blackjack-v1](https://gymnasium.farama.org/environments/toy_text/blackjack/)(1 балл), в частности подберите оптимальную **alpha** для каждой среды.\n",
        "\n",
        "*Задание для \"бесплатных\" баллов :)))*\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def q_learning(env_id, alpha_values, episodes=1000, gamma=0.99, epsilon=0.1):\n",
        "    best_alpha = None\n",
        "    best_reward = -float('inf')\n",
        "    all_rewards = []\n",
        "\n",
        "    for alpha in alpha_values:\n",
        "        env = gym.make(env_id)\n",
        "        state_space = env.observation_space.n\n",
        "        action_space = env.action_space.n\n",
        "\n",
        "        Q = np.zeros((state_space, action_space))\n",
        "        rewards = []\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "\n",
        "            while not done:\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(Q[state])\n",
        "\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "        avg_reward = np.mean(rewards)\n",
        "        all_rewards.append(avg_reward)\n",
        "\n",
        "        if avg_reward > best_reward:\n",
        "            best_reward = avg_reward\n",
        "            best_alpha = alpha\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    return best_alpha, all_rewards\n",
        "\n",
        "alphas = np.linspace(0.01, 1.0, 20)\n",
        "best_alpha_frozen, rewards_frozen = q_learning(\"FrozenLake-v1\", alphas)\n",
        "print(f\"Best alpha for FrozenLake: {best_alpha_frozen:.2f}\")\n",
        "\n",
        "def q_learning_blackjack(alpha_values, episodes=10000, gamma=1.0, epsilon=0.1):\n",
        "    best_alpha = None\n",
        "    best_reward = -float('inf')\n",
        "    all_rewards = []\n",
        "\n",
        "    for alpha in alpha_values:\n",
        "        env = gym.make(\"Blackjack-v1\", sab=True)\n",
        "        Q = {}\n",
        "        rewards = []\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "\n",
        "            while not done:\n",
        "                if state not in Q:\n",
        "                    Q[state] = np.zeros(env.action_space.n)\n",
        "\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(Q[state])\n",
        "\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                if next_state not in Q:\n",
        "                    Q[next_state] = np.zeros(env.action_space.n)\n",
        "\n",
        "                Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "        avg_reward = np.mean(rewards)\n",
        "        all_rewards.append(avg_reward)\n",
        "\n",
        "        if avg_reward > best_reward:\n",
        "            best_reward = avg_reward\n",
        "            best_alpha = alpha\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    return best_alpha, all_rewards\n",
        "\n",
        "best_alpha_blackjack, rewards_blackjack = q_learning_blackjack(alphas)\n",
        "print(f\"Best alpha for Blackjack: {best_alpha_blackjack:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCXB_6iABtsU",
        "outputId": "1524c6c3-6000-4cba-c4d1-2780aad6a0ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best alpha for FrozenLake: 0.74\n",
            "Best alpha for Blackjack: 0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CjetpcrhyO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNu3x2JFrj05"
      },
      "source": [
        "## Задание 3: PPO (5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d289db0"
      },
      "source": [
        "В этом задании вам предстоить реализовать алгоритм PPO. С помощью этого алгоритма тюнили ChatGPT, учили ботов играть в Dota 2, Minecraft, Rocket League и много других игр и не только. В целом считается, что PPO один самых стабильных и \"работающих из коробки\" RL алгоритмов, поэтому его очень любят и ценят практики. Несмотря на концептуальную простоту и множество заслуг, реализация тем не менее требует знания некоторых трюков, которые могут сильно влиять на итоговый результат."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d101972"
      },
      "source": [
        "**Список ссылок на которые стоит смотреть во время выполнения задания:**\n",
        "\n",
        "1. Настоятельно рекомендуется прочесть эту страничку: [Part 3: Intro to Policy Optimization](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)   \n",
        "2. Краткое введение в PPO (прочесть обязательно): [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n",
        "\n",
        "3. Набор советов по тому, как обычно имплементируется PPO (без страшной теории, но читать лучше последним): [The 37 Implementation Details of Proximal Policy Optimization](https://ppo-details.cleanrl.dev//2021/11/05/ppo-implementation-details/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7a2039cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbb638d-470d-4c29-9f69-02ddd9e35023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium imageio tqdm torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d3df89ed"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "from gymnasium.wrappers import ClipAction\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from tqdm.auto import trange, tqdm\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9EUSVHMNqIJ",
        "outputId": "db106aed-8e40-412f-bb9f-364c28c13b12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.2.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.9.0 mujoco-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HnJfGhGruyI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93f91404"
      },
      "source": [
        "### Среда"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c8a794f"
      },
      "source": [
        "Решать будем среду [HalfCheetah-v4](https://gymnasium.farama.org/environments/mujoco/half_cheetah/). Выглядит примерно так:\n",
        "\n",
        "![halfcheetah](https://gymnasium.farama.org/_images/half_cheetah.gif)\n",
        "\n",
        "Наша задача заставить его бежать вперед настолько быстро, насколько это возможно. Среда предполагает continuous действия, размерности 6. Состояния представляют собой вектора размерности 17. Это будет выходными и входными размерностами вашего актора соответственно. Подробнее почитать, за что отвечают каждые размерности можно в документации, но на самом деле это не очень важно (агент сам выучит все что ему надо)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "esZU0bHrs3Gs"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"HalfCheetah-v4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8ab9cbfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d888c779-6d53-4934-94bb-c2c0ebddf9ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-inf, inf, (17,), float64)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "gym.make(ENV_NAME).observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "684939e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d15483-9789-444b-fad2-6c0974373659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-1.0, 1.0, (6,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "gym.make(ENV_NAME).action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31913977"
      },
      "source": [
        "Начнем с того, что определимся с тем, как нам подготовить среду для обучения. Агент будет выдавать нормальное распределение для действий, поэтому нам нужно будет клипать возможные выбросы по значениям [`ClipAction`]. Можно делать TanhNormal, но часто хватает простого клиппинга без усложнений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "427349d3"
      },
      "outputs": [],
      "source": [
        "def make_env(env_name: str) -> gym.Env:\n",
        "    \"\"\"\n",
        "    Создает и оборачивает среду, применяя нормализацию действий.\n",
        "\n",
        "    Args:\n",
        "        env_name (str): Название среды Gym для создания.\n",
        "\n",
        "    Returns:\n",
        "        gym.Env: Инициализированная среда с примененным обертыванием ClipAction.\n",
        "    \"\"\"\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    env = ClipAction(env)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2787671f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5745331a-e9f7-4f39-94c0-90dc75cb3986"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ClipAction<TimeLimit<OrderEnforcing<PassiveEnvChecker<HalfCheetahEnv<HalfCheetah-v4>>>>>>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "make_env(ENV_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jky_M0BttLP0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "310bc246"
      },
      "source": [
        "### Актор"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "184bb73e"
      },
      "source": [
        "В данном случае актор - это сущность которая взаимодействуе со средой. Он должен предсказывать распределение действий $\\pi(a | s)$, а также будущую среднюю награду из этого состояния - $V(s)$.\n",
        "\n",
        "Таким образом, мы ожидаем, что актор должен уметь:\n",
        "\n",
        "1. Предсказывать распределение действий. Не забудьте что $\\sigma$ должна быть неотрицательной. Этого можно достичь если обучать $\\log \\sigma$ и в нужный момент делать `torch.exp(log_sigma)`\n",
        "\n",
        "2. Предсказывать награду\n",
        "\n",
        "Нам еще понадобится метод для сэмплирования действия. Во время обучения он не используется, но полезен для оценки агента и будущего инференса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "da07538f"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 32):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            obs_dim (int): Размерность пространства наблюдений.\n",
        "            action_dim (int): Размерность пространства действий.\n",
        "            hidden_dim (int, optional): Количество нейронов в скрытых слоях.\n",
        "        \"\"\"\n",
        "        self.actor_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "        self.critic_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_action_dist(self, obs: torch.Tensor) -> Normal:\n",
        "        \"\"\"\n",
        "        Возвращает распределение действий для заданного состояния.\n",
        "\n",
        "        Args:\n",
        "            obs (torch.Tensor): Тензор наблюдений.\n",
        "\n",
        "        Returns:\n",
        "            Normal: Распределение действий.\n",
        "        \"\"\"\n",
        "        mean = self.actor_net(obs)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return Normal(mean, std)\n",
        "\n",
        "\n",
        "    def get_value(self, obs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Возвращает оценку ценности заданного состояния.\n",
        "\n",
        "        Args:\n",
        "            obs (torch.Tensor): Тензор наблюдений.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Оценка значения состояния.\n",
        "        \"\"\"\n",
        "        return self.critic_net(obs).squeeze(-1)\n",
        "\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> Tuple[Normal, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Прямой проход.\n",
        "\n",
        "        Args:\n",
        "            obs (torch.Tensor): Тензор наблюдений.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Normal, torch.Tensor]:\n",
        "                - Распределение действий (Normal).\n",
        "                - Оценка значения состояния (torch.Tensor).\n",
        "        \"\"\"\n",
        "        dist = self.get_action_dist(obs)\n",
        "        value = self.get_value(obs)\n",
        "        return dist, value\n",
        "\n",
        "\n",
        "    def get_action(self, obs: torch.Tensor,\n",
        "                   greedy: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Возвращает действие на основе текущей политики.\n",
        "\n",
        "        Args:\n",
        "            obs (torch.Tensor): Тензор наблюдений, размерности.\n",
        "            greedy (bool, optional): Флаг, указывающий на использование жадной стратегии.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Выбранное действие.\n",
        "        \"\"\"\n",
        "        dist = self.get_action_dist(obs)\n",
        "        if greedy:\n",
        "            return dist.mean\n",
        "        else:\n",
        "            return dist.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "77dba816"
      },
      "outputs": [],
      "source": [
        "# sanity checks\n",
        "test_actor_ = Actor(obs_dim=17, action_dim=6, hidden_dim=32)\n",
        "test_obs_ = torch.randn(5, 17)\n",
        "\n",
        "assert test_actor_.get_action_dist(test_obs_).loc.shape == (5, 6)\n",
        "assert test_actor_.get_action_dist(test_obs_).scale.shape == (5, 6)\n",
        "assert test_actor_.get_action_dist(test_obs_).sample().shape == (5, 6)\n",
        "\n",
        "assert test_actor_.get_value(test_obs_).shape == (5,)\n",
        "assert len(test_actor_(test_obs_)) == 2\n",
        "\n",
        "assert torch.isclose(\n",
        "    test_actor_.get_action(test_obs_, greedy=True),\n",
        "    test_actor_.get_action(test_obs_, greedy=True)\n",
        ").sum()\n",
        "\n",
        "assert not torch.isclose(\n",
        "    test_actor_.get_action(test_obs_, greedy=False),\n",
        "    test_actor_.get_action(test_obs_, greedy=False)\n",
        ").sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l588XTwa3oPm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16e45077"
      },
      "source": [
        "### Награда\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb363e9"
      },
      "source": [
        "Для PPO нам нужно уметь оценивать returns траектории $R(\\tau)$, что на самом деле то же самое, что $Q(s, a)$ - будущая награда в состоянии $s_{t}$ при совершении действия $a_{t}$. Существует несколько способов это посчитать, самый простой и наивный, воспользоваться следующим свойством: $$Q(s, a) = r_t + \\gamma V(s_{t + 1})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQPiAyeGEtr"
      },
      "source": [
        "\n",
        "Важно также помнить, что в случае когда среда завершилась (`done`), будущая награда равна 0, поэтому нужно это учитывать во всех подсчетах. Это легко сделать домножив будущую награду на (`1 - done`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg9AB-InGGo_"
      },
      "source": [
        "Т.к. дальше мы будем работать с векторизованными средами, функции должны работать сразу с батчами:\n",
        "\n",
        "  \n",
        "\n",
        "*   **rewards**: `[num_env_steps, batch_size]`   \n",
        "*   **values**: `[num_env_steps + 1, batch_size]`\n",
        "*   **dones**: `[num_env_steps, batch_size]`\n",
        "\n",
        "\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "59555d07"
      },
      "outputs": [],
      "source": [
        "def one_step_returns(rewards: torch.Tensor,\n",
        "                     values: torch.Tensor,\n",
        "                     dones: torch.Tensor,\n",
        "                     gamma: float = 0.99) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисляет one-step returns.\n",
        "\n",
        "    Args:\n",
        "        rewards (torch.Tensor): Тензор наград.\n",
        "        values (torch.Tensor): Тензор оценок значений состояний.\n",
        "        dones (torch.Tensor): Бинарные флаги окончания эпизода.\n",
        "        gamma (float, optional): Дисконтный множитель.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: One-step returns.\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(rewards) + 1 == len(values), \"values should contain 1 more estimate for the final state\"\n",
        "\n",
        "    # считаем r + V(s_next) для всех примеров сразу\n",
        "    returns = rewards + (1 - dones) * gamma * values[1:]\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1cd2b67"
      },
      "source": [
        "Может возникнуть вопрос: *откуда мы возьмем $V(s)$?*\n",
        "\n",
        "Его будем предсказывать с помощью агента во время сбора данных и сохранять. Нюанс в том, что в начале обучения $V(s)$ будет далека от реальной value функции агента, но со временем сойдется. Тем не менее, тот variance, что возникнет из-за неточных предсказаний, может привести к тому, что агент разойдется с самого начала. Поэтому лучше считать награду по всей траектории явно, а вот последнее состояние (если траектория частичная) оценивать с помощью $V(s_{last})$. Как раз поэтому $V(\\cdot)$ содержит в себе на один больше шагов. Это пригодится для оценки последнего состояния (мы должны знать для него следующую оценку)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60krQPKUGcFL"
      },
      "source": [
        "\n",
        "Вам нужно посчитать: $$\\sum_{i=t}^{T-1} \\gamma^{i - t} r_i + \\gamma^{T - t}V(s_T)$$\n",
        "\n",
        "То есть с каждого шага $t$ посчитать будущую дисконтированную награду до конца эпизода (или до конца собранного промежутка, а в конце оценить с помощью $V$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWBlFvAbGsNm"
      },
      "source": [
        "Это можно сделать наивно за $O(n^2)$, но подумайте как сделать это **за один проход**. Не забывайте учитывать `done` т.к. он и определяет конец эпизода.\n",
        "\n",
        "\n",
        "*Подсказка: попробуйте считать с конца, рекуррентно*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "068e6d6f"
      },
      "outputs": [],
      "source": [
        "def multi_step_returns(rewards: torch.Tensor,\n",
        "                       values: torch.Tensor,\n",
        "                       dones: torch.Tensor,\n",
        "                       gamma: float = 0.99) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисляет multi-step returns.\n",
        "\n",
        "    Args:\n",
        "        rewards (torch.Tensor): Тензор наград [T, B].\n",
        "        values (torch.Tensor): Тензор оценок состояний [T+1, B].\n",
        "        dones (torch.Tensor): Тензор флагов завершения эпизодов [T, B].\n",
        "        gamma (float): Дисконтирующий множитель.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Тензор returns [T, B].\n",
        "    \"\"\"\n",
        "    T = rewards.shape[0]\n",
        "    returns = torch.zeros_like(rewards)\n",
        "\n",
        "    next_return = values[-1]\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        next_return = rewards[t] + gamma * next_return * (1 - dones[t])\n",
        "        returns[t] = next_return\n",
        "\n",
        "    return returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8873f1b1"
      },
      "outputs": [],
      "source": [
        "# simple determenistic checks\n",
        "test_rewards_ = torch.tensor([0.1, 0.2, -0.01, 0.5, 4.0, -1.0])\n",
        "test_values_ = torch.tensor([0.1, 0.3, 0.15, 0.3, 0.4, 0.6, 0.5])\n",
        "test_dones_ = torch.tensor([0, 0, 0, 1, 0, 0])\n",
        "\n",
        "test_answer_ = torch.tensor([ 0.7733,  0.6802,  0.4850,  0.5000,  3.5000, -0.5050])\n",
        "\n",
        "assert torch.all(torch.isclose(\n",
        "    multi_step_returns(test_rewards_, test_values_, test_dones_).round(decimals=4),\n",
        "    test_answer_\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4BHDNvnDpk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af69d8fb"
      },
      "source": [
        "### Сбор данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87864411"
      },
      "source": [
        "Как мы обсуждали в части про `StableBaselines3`, собирать траектории последовательно не оптимально, гораздо быстрее делать это одновременно, собирая все состояния в один батч для агента и получать все действия на этом шаге за раз.\n",
        "\n",
        "Сделать это достаточно просто (нам хватит наивной реализации, которая последовательно делает step в N средах):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e4c1d518"
      },
      "outputs": [],
      "source": [
        "from gymnasium.vector import SyncVectorEnv\n",
        "\n",
        "vec_env = SyncVectorEnv(\n",
        "    [lambda: make_env(ENV_NAME) for _ in range(10)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "24876ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bbc83f-54cd-46b2-bef4-8579109ff30f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-inf, inf, (10, 17), float64)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "vec_env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "58323e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1f77bf-fc38-4804-b46a-c556033a3c9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-inf, inf, (10, 6), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "vec_env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1c6adb8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489dd385-9d3b-4d63-95a9-d3a2fddb621f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "vec_env.reset()[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54f2ad5f"
      },
      "source": [
        "Как можно заметить, теперь появилась дополнительная размерность размера 10 (количество сред). Схематически обучение в таком случае выглядит так:\n",
        "\n",
        "```python\n",
        "envs = VecEnv(num_envs=N)\n",
        "actor = Actor()\n",
        "data = []\n",
        "\n",
        "obs = envs.reset()\n",
        "for update in range(1, total_timesteps // (N*M)):\n",
        "  # ROLLOUT PHASE\n",
        "  for step in range(0, M):\n",
        "      action, action_logprob, value = actor.get_action_and_logprob_and_value(obs)\n",
        "      next_obs, reward, done, info = envs.step(action) # step in N environments\n",
        "      data.append([obs, action, reward, done, value, action_logprob, some_other_stuff]) # store data\n",
        "      \n",
        "      obs = next_obs\n",
        "\n",
        "  # LEARNING PHASE\n",
        "  for epoch in range(num_epochs):\n",
        "      batch = sample(data)\n",
        "      ppo_loss(actor, batch) # len(data) = N*M\n",
        "      # step optimizer here\n",
        "```\n",
        "\n",
        "Плюсы такой имплементации в более быстром сборе данных, а также в их разнообразии, т.к. мы одновременно получаем опыт в средах с разной инициализацией."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oas9TD7UJ6HZ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405142b0"
      },
      "source": [
        "### PPO Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f9c09b5"
      },
      "source": [
        "Вначале напишем PPO лосс, после чего сам train loop. Лосс PPO состоит из трех частей: лосса актора, лосса критика и обычно добавлют энтропийный лосс, чтобы поощрять exploration (чтобы энтропия распределения действий не падала)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "345bf01a"
      },
      "source": [
        "Лосс критика представляет собой обычный MSE лосс между предсказаниями текущего критика и таргетом - настоящими наградами, их мы уже научились считать выше."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "c9059053"
      },
      "outputs": [],
      "source": [
        "def critic_loss(target_values: torch.Tensor,\n",
        "                pred_values: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисляет loss для критика.\n",
        "\n",
        "    Args:\n",
        "        target_values (torch.Tensor): Целевые значения, вычисленные с помощью multi-step returns.\n",
        "        pred_values (torch.Tensor): Предсказания критика для соответствующих состояний.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Значение loss для критика.\n",
        "    \"\"\"\n",
        "    return torch.nn.functional.mse_loss(pred_values, target_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ca0bb2"
      },
      "source": [
        "Лосс актора в точности следует формуле, которую вы можете найти (и лучше прочитать интерпретацию) [тут](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "55bc1798"
      },
      "outputs": [],
      "source": [
        "def actor_loss(new_log_probs: torch.Tensor,\n",
        "               old_log_probs: torch.Tensor,\n",
        "               advantages: torch.Tensor,\n",
        "               eps: float = 0.2) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисляет loss для актора.\n",
        "\n",
        "    Args:\n",
        "        new_log_probs (torch.Tensor): Логарифм вероятностей действий согласно текущей политики.\n",
        "        old_log_probs (torch.Tensor): Логарифм вероятностей действий согласно политике, которой собирались данные.\n",
        "        advantages (torch.Tensor): Оценки A(s, a) для пар состояние-действие.\n",
        "        eps (float, optional): Параметр epsilon из PPO (см. Open AI Spinning Up про PPO).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Значение loss для актора.\n",
        "    \"\"\"\n",
        "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "    unclipped = ratio * advantages\n",
        "    clipped = torch.clamp(ratio, 1 - eps, 1 + eps) * advantages\n",
        "    loss = -torch.mean(torch.min(unclipped, clipped))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "03ae3412"
      },
      "outputs": [],
      "source": [
        "def entropy_loss(action_dist: Normal) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисляет loss на основе энтропии распределения действий.\n",
        "\n",
        "    Args:\n",
        "        action_dist (Normal): Распределение действий, параметризованное актором.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Энтропия распределения действий.\n",
        "    \"\"\"\n",
        "    entropy = action_dist.entropy()\n",
        "    return -entropy.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYpjJNskng2N"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cb5651e"
      },
      "source": [
        "### Оценка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH1ae9VyMhSh"
      },
      "source": [
        "Немного кода для удобства оценки и визуализации траекторий"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "682efd1c"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def rollout(env: gym.Env, actor: Actor, seed: int, device: str = \"cpu\") -> float:\n",
        "    \"\"\"\n",
        "    Выполняет один rollout в среде с использованием текущей политики актора.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Среда Gym, в которой происходит rollout.\n",
        "        actor (Actor): Политика, используемая для выбора действий.\n",
        "        seed (int): Сид для инициализации среды.\n",
        "        device (str, optional): Устройство, на котором будут выполняться вычисления.\n",
        "\n",
        "    Returns:\n",
        "        float: Суммарная награда, полученная за весь эпизод.\n",
        "    \"\"\"\n",
        "\n",
        "    obs, done = env.reset(seed=seed)[0], False\n",
        "    total_reward = 0.0\n",
        "\n",
        "    while not done:\n",
        "        obs = torch.tensor(obs, dtype=torch.float, device=device).reshape(1, -1)\n",
        "        action = actor.get_action(obs, greedy=True)\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(action.flatten().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "    return total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC7-YZcONNwT"
      },
      "source": [
        "Хорошим тоном считается фиксировать сиды среды для оценки, чтобы можно было сравнивать разные гипепараметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fBLZED5VNMwc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(actor: Actor,\n",
        "             num_episodes: int,\n",
        "             seed: int, device: str = \"cpu\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Оценивает производительность актора, выполняя несколько эпизодов в среде.\n",
        "\n",
        "    Args:\n",
        "        actor (Actor): Оцениваемая политика.\n",
        "        num_episodes (int): Количество эпизодов для оценки.\n",
        "        seed (int): Базовый сид для инициализации среды (для воспроизводимости).\n",
        "        device (str, optional): Устройство, на котором будут выполняться вычисления.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Массив формы со средними наградами за каждый эпизод.\n",
        "    \"\"\"\n",
        "    env = gym.make(ENV_NAME)\n",
        "\n",
        "    returns = np.zeros(num_episodes)\n",
        "    for e in range(num_episodes):\n",
        "        returns[e] = rollout(env, actor, seed=seed + e, device=device)\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wfIIAtnk4E"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111824db"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1cfcbd"
      },
      "source": [
        "Напишем игрушечный трейнер. Вначале внимательно прочитайте весь код, чтобы понять глобально, что происходит. Ниже также можете посмотреть пример использования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "09cb3a96"
      },
      "outputs": [],
      "source": [
        "def compute_gae(rewards, values, dones, gamma=0.99, gae_lambda=0.95):\n",
        "    num_steps, num_envs = rewards.shape\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    last_gae = 0\n",
        "\n",
        "    for t in reversed(range(num_steps)):\n",
        "        mask = 1.0 - dones[t]\n",
        "        delta = rewards[t] + gamma * values[t + 1] * mask - values[t]\n",
        "        advantages[t] = last_gae = delta + gamma * gae_lambda * mask * last_gae\n",
        "\n",
        "    returns = advantages + values[:-1]\n",
        "    return returns, advantages\n",
        "\n",
        "\n",
        "class PPOTrainer:\n",
        "    def __init__(\n",
        "            self,\n",
        "            total_timesteps: int = 1_000_000,\n",
        "            num_vec_envs: int = 8,\n",
        "            num_env_steps: int = 256,\n",
        "            num_update_epochs: int = 5,\n",
        "            batch_size: int = 64,\n",
        "            gamma: float = 0.99,\n",
        "            eps: float = 0.2,\n",
        "    ):\n",
        "        self.train_env = SyncVectorEnv(\n",
        "            [lambda: make_env(ENV_NAME) for _ in range(num_vec_envs)]\n",
        "        )\n",
        "        self.total_updates = round(total_timesteps / (num_env_steps * num_vec_envs))\n",
        "\n",
        "        # set up Actor and optimizer\n",
        "        self.actor = Actor(\n",
        "            obs_dim=self.train_env.single_observation_space.shape[-1],\n",
        "            action_dim=self.train_env.single_action_space.shape[-1],\n",
        "            hidden_dim=64\n",
        "        ).to(DEVICE)\n",
        "        self.optim = Adam(self.actor.parameters(), lr=3e-4, eps=1e-5)\n",
        "        self.lr_scheduler = LinearLR(\n",
        "            self.optim, start_factor=1.0, end_factor=0.0, total_iters=self.total_updates\n",
        "        )\n",
        "        self.eps = eps\n",
        "        self.gamma = gamma\n",
        "        self.num_env_steps = num_env_steps\n",
        "        self.num_vec_envs = num_vec_envs\n",
        "        self.num_update_epochs = num_update_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def update_epochs(self, observations, actions, log_probs, returns, advantages):\n",
        "        all_idxs = np.arange(self.num_vec_envs * self.num_env_steps)\n",
        "\n",
        "        for epoch in range(self.num_update_epochs):\n",
        "            np.random.shuffle(all_idxs)\n",
        "            for start in range(0, len(all_idxs), self.batch_size):\n",
        "                batch_idxs = all_idxs[start:start + self.batch_size]\n",
        "\n",
        "                batch_obs = observations[batch_idxs]\n",
        "                batch_actions = actions[batch_idxs]\n",
        "                batch_old_log_probs = log_probs[batch_idxs]\n",
        "                batch_returns = returns[batch_idxs]\n",
        "                batch_advantages = advantages[batch_idxs]\n",
        "\n",
        "                action_dist = self.actor.get_action_distribution(batch_obs)\n",
        "                new_log_probs = action_dist.log_prob(batch_actions).sum(-1)\n",
        "                entropy_l = entropy_loss(action_dist)\n",
        "                value_preds = self.actor.get_value(batch_obs).squeeze()\n",
        "\n",
        "                actor_l = actor_loss(new_log_probs, batch_old_log_probs, batch_advantages, eps=self.eps)\n",
        "                critic_l = critic_loss(batch_returns, value_preds)\n",
        "\n",
        "                loss = actor_l + 0.5 * critic_l + 0.001 * entropy_l\n",
        "\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
        "                self.optim.step()\n",
        "\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        obs_shape = self.train_env.single_observation_space.shape[-1]\n",
        "        action_shape = self.train_env.single_action_space.shape[-1]\n",
        "\n",
        "        observations = torch.zeros((self.num_env_steps, self.num_vec_envs, obs_shape), device=DEVICE)\n",
        "        actions = torch.zeros((self.num_env_steps, self.num_vec_envs, action_shape), device=DEVICE)\n",
        "        log_probs = torch.zeros(self.num_env_steps, self.num_vec_envs, device=DEVICE)\n",
        "        rewards = torch.zeros(self.num_env_steps, self.num_vec_envs, device=DEVICE)\n",
        "\n",
        "        values = torch.zeros(self.num_env_steps + 1, self.num_vec_envs, device=DEVICE)\n",
        "        dones = torch.zeros(self.num_env_steps, self.num_vec_envs, device=DEVICE)\n",
        "\n",
        "        obs = torch.tensor(self.train_env.reset(seed=10)[0], dtype=torch.float, device=DEVICE)\n",
        "        for update in trange(self.total_updates, desc=\"Updates\"):\n",
        "            for step in range(self.num_env_steps):\n",
        "                with torch.no_grad():\n",
        "                    action_dist = self.actor.get_action_distribution(obs)\n",
        "                    action = action_dist.sample()\n",
        "                    log_prob = action_dist.log_prob(action).sum(-1)\n",
        "                    value = self.actor.get_value(obs)\n",
        "\n",
        "                next_obs, reward, terminated, truncated, infos = self.train_env.step(action.cpu().numpy())\n",
        "\n",
        "                observations[step] = obs.to(DEVICE)\n",
        "                actions[step] = action.to(DEVICE)\n",
        "                log_probs[step] = log_prob.to(DEVICE)\n",
        "                rewards[step] = torch.tensor(reward, device=DEVICE)\n",
        "                values[step] = value.squeeze().to(DEVICE)\n",
        "                dones[step] = torch.tensor(terminated, device=DEVICE)\n",
        "\n",
        "                obs = torch.tensor(next_obs, dtype=torch.float, device=DEVICE)\n",
        "            else:\n",
        "                values[-1] = self.actor.get_value(obs).squeeze()\n",
        "\n",
        "            returns, advantages = compute_gae(\n",
        "                rewards, values, dones, gamma=self.gamma, gae_lambda=0.95\n",
        "            )\n",
        "\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "            self.update_epochs(\n",
        "                observations.flatten(0, 1),\n",
        "                actions.flatten(0, 1),\n",
        "                log_probs.flatten(),\n",
        "                returns.flatten(),\n",
        "                advantages.flatten()\n",
        "            )\n",
        "\n",
        "            if update % 50 == 0:\n",
        "                eval_returns = evaluate(self.actor, 10, seed=42, device=DEVICE)\n",
        "                tqdm.write(f\"Mean return: {eval_returns.mean()}\")\n",
        "\n",
        "        return self.actor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOTrainer:\n",
        "    def __init__(\n",
        "            self,\n",
        "            total_timesteps: int = 1_000_000,\n",
        "            num_vec_envs: int = 8,\n",
        "            num_env_steps: int = 256,\n",
        "            num_update_epochs: int = 5,\n",
        "            batch_size: int = 64,\n",
        "            gamma: float = 0.99,\n",
        "            eps: float = 0.2,\n",
        "    ):\n",
        "        self.train_env = SyncVectorEnv(\n",
        "            [lambda: make_env(ENV_NAME) for _ in range(num_vec_envs)]\n",
        "        )\n",
        "        self.total_updates = round(total_timesteps / (num_env_steps * num_vec_envs))\n",
        "\n",
        "        self.actor = Actor(\n",
        "            obs_dim=self.train_env.single_observation_space.shape[-1],\n",
        "            action_dim=self.train_env.single_action_space.shape[-1],\n",
        "            hidden_dim=64\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        self.optim = Adam(self.actor.parameters(), lr=3e-4, eps=1e-5)\n",
        "        self.lr_scheduler = LinearLR(\n",
        "            self.optim, start_factor=1.0, end_factor=0.0, total_iters=self.total_updates\n",
        "        )\n",
        "        self.eps = eps\n",
        "        self.gamma = gamma\n",
        "        self.num_env_steps = num_env_steps\n",
        "        self.num_vec_envs = num_vec_envs\n",
        "        self.num_update_epochs = num_update_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def update_epochs(self, observations, actions, log_probs, returns, advantages):\n",
        "        all_idxs = np.arange(self.num_vec_envs * self.num_env_steps)\n",
        "\n",
        "        for epoch in range(self.num_update_epochs):\n",
        "            np.random.shuffle(all_idxs)\n",
        "            for start in range(0, len(all_idxs), self.batch_size):\n",
        "                batch_idxs = all_idxs[start:start + self.batch_size]\n",
        "\n",
        "                batch_obs = observations[batch_idxs]\n",
        "                batch_actions = actions[batch_idxs]\n",
        "                batch_old_log_probs = log_probs[batch_idxs]\n",
        "                batch_returns = returns[batch_idxs]\n",
        "                batch_advantages = advantages[batch_idxs]\n",
        "\n",
        "                action_dist, _ = self.actor(batch_obs)\n",
        "                new_log_probs = action_dist.log_prob(batch_actions).sum(-1)\n",
        "                entropy_l = entropy_loss(action_dist)\n",
        "                value_preds = self.actor.get_value(batch_obs).squeeze()\n",
        "\n",
        "                actor_l = actor_loss(new_log_probs, batch_old_log_probs, batch_advantages, eps=self.eps)\n",
        "                critic_l = critic_loss(batch_returns, value_preds)\n",
        "\n",
        "                loss = actor_l + 0.5 * critic_l + 0.001 * entropy_l\n",
        "\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
        "                self.optim.step()\n",
        "\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        obs_shape = self.train_env.single_observation_space.shape[-1]\n",
        "        action_shape = self.train_env.single_action_space.shape[-1]\n",
        "\n",
        "        observations = torch.zeros((self.num_env_steps, self.num_vec_envs, obs_shape), device=DEVICE)\n",
        "        actions = torch.zeros((self.num_env_steps, self.num_vec_envs, action_shape), device=DEVICE)\n",
        "        log_probs = torch.zeros(self.num_env_steps, self.num_vec_envs, device=DEVICE)\n",
        "        rewards = torch.zeros(self.num_env_steps, self.num_vec_envs, device=DEVICE)\n",
        "\n",
        "        values = torch.zeros(self.num_env_steps + 1, self.num_vec_envs, device=DEVICE)\n",
        "        dones = torch.zeros(self.num_env_steps, self.num_vec_envs, device=DEVICE)\n",
        "\n",
        "        obs = torch.tensor(self.train_env.reset(seed=10)[0], dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        for update in trange(self.total_updates, desc=\"Updates\"):\n",
        "            for step in range(self.num_env_steps):\n",
        "                with torch.no_grad():\n",
        "                    action_dist, _ = self.actor(obs)\n",
        "                    action = action_dist.sample()\n",
        "                    log_prob = action_dist.log_prob(action).sum(-1)\n",
        "                    value = self.actor.get_value(obs)\n",
        "\n",
        "                next_obs, reward, terminated, truncated, infos = self.train_env.step(action.cpu().numpy())\n",
        "\n",
        "                observations[step] = obs\n",
        "                actions[step] = action\n",
        "                log_probs[step] = log_prob\n",
        "                rewards[step] = torch.tensor(reward, device=DEVICE)\n",
        "                values[step] = value.squeeze()\n",
        "                dones[step] = torch.tensor(terminated, device=DEVICE)\n",
        "\n",
        "                obs = torch.tensor(next_obs, dtype=torch.float, device=DEVICE)\n",
        "            else:\n",
        "                values[-1] = self.actor.get_value(obs).squeeze()\n",
        "\n",
        "            returns, advantages = compute_gae(\n",
        "                rewards, values, dones, gamma=self.gamma, gae_lambda=0.95\n",
        "            )\n",
        "\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "            self.update_epochs(\n",
        "                observations.flatten(0, 1).clone().detach(),\n",
        "                actions.flatten(0, 1).clone().detach(),\n",
        "                log_probs.flatten().clone().detach(),\n",
        "                returns.flatten().clone().detach(),\n",
        "                advantages.flatten().clone().detach()\n",
        "\n",
        "            )\n",
        "\n",
        "            if update % 50 == 0:\n",
        "                eval_returns = evaluate(self.actor, 10, seed=42, device=DEVICE)\n",
        "                tqdm.write(f\"Mean return: {eval_returns.mean()}\")\n",
        "\n",
        "        return self.actor\n"
      ],
      "metadata": {
        "id": "QSvzUvD4SnXa"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXJg5uMiPbAH"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839c7fb8"
      },
      "source": [
        "### Обучение и визуализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3bd099a"
      },
      "source": [
        "Попробуем обучить агента на 5М транзиций. Я постаралась зафиксировать сиды, так чтобы при каждом запуске с идентичными параметрами получался тот же результат. Хорошим результатом считается 10к+ награды, но т.к. полученная имплементация во многих смыслах все еще игрушечная, нам будет достаточно 3к"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifS4onNZjDqe"
      },
      "source": [
        "При следующих параметрах правильная имплементация набирает 4к+ награды. Вы можете поэксперементировать и попробовать свои гиперпараметры (включая те, что я захардкодила в трейнере). Учтите, что RL алгоритмы гораздо менее стабильны, чем в среднем в DL, и при неправильных параметрах даже правильная имплементация может не обучаться ;)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "a25df008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "687c9d44fbc44ae4856692e93aebaefb",
            "35624ee08e2444ec8673ef471e6f44c5",
            "5439a68cc9d1428b873dcf28ff8ec9d6",
            "61dd155df250401b93b65575a10a4b63",
            "d30ffa1e66a64c7b8a65df95e8af0fcf",
            "1a2aec7d076d440da4a6d70491470807",
            "5609ae9300f9472c9ecc6dd9a7ca5f46",
            "5f04c935fc7f4982a770a3609c163617",
            "8c017abc424d4fb1bc7d9c4fa17ca377",
            "ec6824727988478cba29919a1e2a9b78",
            "4a5f2d908aa84788b962bc258a35be36"
          ]
        },
        "outputId": "a541c35e-70c1-4678-ddbd-e10d76781424"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Updates:   0%|          | 0/625 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "687c9d44fbc44ae4856692e93aebaefb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean return: 52.335170248842346\n",
            "Mean return: 650.2495230830858\n",
            "Mean return: 1896.0188906281414\n",
            "Mean return: 2251.9263317587183\n",
            "Mean return: 2719.7345617423557\n",
            "Mean return: 3123.309076321365\n",
            "Mean return: 3322.208690788895\n",
            "Mean return: 3344.668231542447\n",
            "Mean return: 3447.354252179595\n",
            "Mean return: 3555.764373885058\n",
            "Mean return: 3656.5754626780863\n",
            "Mean return: 3753.588845542085\n",
            "Mean return: 3771.6704986873024\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "trainer = PPOTrainer(\n",
        "    total_timesteps=5_000_000,\n",
        "    batch_size=64,\n",
        "    num_update_epochs=3,\n",
        "    num_env_steps=1000\n",
        ")\n",
        "trained_actor = trainer.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a759360c"
      },
      "source": [
        "После обучения иногда интересно посмотреть визуально, что агент выучил. Для этого в gym можно указать `render_mode` при инициализации среды. Там есть несколько опций, нам пригодится та, что возвращает в конце список картинок в numpy. Дальше мы можем их зарендерить и отразить с помощью `imageio`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53b12e27"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(ENV_NAME, render_mode=\"rgb_array_list\")\n",
        "\n",
        "rollout(eval_env, trained_actor, seed=32)\n",
        "imageio.mimsave(\"rollout.mp4\", eval_env.render(), fps=16, format=\"mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46095e5b"
      },
      "source": [
        "В идеале он должен быстро бежать\n",
        "\n",
        "*(не обязательно на ногах, иногда может и на спине...)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bb7cc0f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "Video(\"rollout.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bebf2909"
      },
      "outputs": [],
      "source": [
        "!rm rollout.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZaoDyEwQH-5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnQHs6iGdeFh"
      },
      "source": [
        "# Домашнее задание № 14\n",
        "\n",
        "Выполните все задания в этом ноутбуке\n",
        "\n",
        "+ Мягкий дедлайн: `27.05.25 23:59`\n",
        "+ Жесткий дедлайн: `03.06.25 23:59` (половина баллов)\n",
        "\n",
        "\n",
        "После жесткого дедлайна задание не принимается."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VyLk8-uYhzy1"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0d2028f83c04af5a2689a629bcf7327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa34922a482249298214b7c4d857d504",
              "IPY_MODEL_fbb68225643e4e91ab199ab4d1fd2f2c",
              "IPY_MODEL_9c87ace7df6d47488889865866031bff"
            ],
            "layout": "IPY_MODEL_429624396b1f478bb12de2214744043f"
          }
        },
        "fa34922a482249298214b7c4d857d504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b457cc115720488f823d9629cb67c4a5",
            "placeholder": "​",
            "style": "IPY_MODEL_282eea743ba2424aa824cda7e3225126",
            "value": "Epochs: 100%"
          }
        },
        "fbb68225643e4e91ab199ab4d1fd2f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7daeb413355246eaa0f6125182a9bc8e",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8a9fb652fea4ff6b37f7f821bb45628",
            "value": 200
          }
        },
        "9c87ace7df6d47488889865866031bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00394cb67edd4c05b51665c5f23c7829",
            "placeholder": "​",
            "style": "IPY_MODEL_bf75b5a1526948e6bdfa32fefbe968f1",
            "value": " 200/200 [00:41&lt;00:00,  5.99it/s]"
          }
        },
        "429624396b1f478bb12de2214744043f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b457cc115720488f823d9629cb67c4a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282eea743ba2424aa824cda7e3225126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7daeb413355246eaa0f6125182a9bc8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a9fb652fea4ff6b37f7f821bb45628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00394cb67edd4c05b51665c5f23c7829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf75b5a1526948e6bdfa32fefbe968f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f25c7945b82b4da4bddefe2d4bf6384f": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_fec07d495b034e01af7dda7e98747be5",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m  99%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m99,216/100,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m4,112 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  99%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">99,216/100,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">4,112 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "fec07d495b034e01af7dda7e98747be5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8791418dcca4af88dad9a49319a7413": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c4d6dfd41e094225b727b3decbf97b65",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m  99%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m99,368/100,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m3,849 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  99%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">99,368/100,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">3,849 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c4d6dfd41e094225b727b3decbf97b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "687c9d44fbc44ae4856692e93aebaefb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35624ee08e2444ec8673ef471e6f44c5",
              "IPY_MODEL_5439a68cc9d1428b873dcf28ff8ec9d6",
              "IPY_MODEL_61dd155df250401b93b65575a10a4b63"
            ],
            "layout": "IPY_MODEL_d30ffa1e66a64c7b8a65df95e8af0fcf"
          }
        },
        "35624ee08e2444ec8673ef471e6f44c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a2aec7d076d440da4a6d70491470807",
            "placeholder": "​",
            "style": "IPY_MODEL_5609ae9300f9472c9ecc6dd9a7ca5f46",
            "value": "Updates: 100%"
          }
        },
        "5439a68cc9d1428b873dcf28ff8ec9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f04c935fc7f4982a770a3609c163617",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c017abc424d4fb1bc7d9c4fa17ca377",
            "value": 625
          }
        },
        "61dd155df250401b93b65575a10a4b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec6824727988478cba29919a1e2a9b78",
            "placeholder": "​",
            "style": "IPY_MODEL_4a5f2d908aa84788b962bc258a35be36",
            "value": " 625/625 [46:11&lt;00:00,  4.26s/it]"
          }
        },
        "d30ffa1e66a64c7b8a65df95e8af0fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2aec7d076d440da4a6d70491470807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5609ae9300f9472c9ecc6dd9a7ca5f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f04c935fc7f4982a770a3609c163617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c017abc424d4fb1bc7d9c4fa17ca377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec6824727988478cba29919a1e2a9b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5f2d908aa84788b962bc258a35be36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}